{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "imdb LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPC6mdpdd3nDnRtprRmaS+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyxdev/MyColabNotebooks/blob/main/imdb_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLYYoiApB_Np",
        "outputId": "eef2149b-b7fb-4fa0-9a0b-6f684642e2d5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Embedding, Dense, Activation, LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "model_name = \"lstm_imdb_model.h5\"\n",
        "gdrive_path = \"/content/gdrive\"\n",
        "path = \"MyDrive\"\n",
        "\n",
        "drive.mount(gdrive_path)\n",
        "\n",
        "model_path = os.path.join(gdrive_path, path, model_name)\n",
        "print(\"Chemin du model_path : \", model_path)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Chemin du model_path :  /content/gdrive/MyDrive/lstm_imdb_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE_vLhu9qTL_",
        "outputId": "2a82f3c3-f7d4-4ba5-8102-945dbc9ca1a1"
      },
      "source": [
        "maxf = 1000\n",
        "maxlen = 80\n",
        "((x_train, y_train), (x_test, y_test)) = imdb.load_data(num_words=maxf)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(y_train[:5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYNB4J-CDlrH",
        "outputId": "3cd6e2fe-0619-41ab-9069-fae9291eb098"
      },
      "source": [
        "if os.path.exists(model_path):\n",
        "  print(\"{} trouvé !\".format(model_path))\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "else:\n",
        "  print(\"{} non trouvé ! Redefinition et entrainement ...\".format(model_path))\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(maxf, 128, input_length=maxlen))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(x_train, y_train, epochs=5)\n",
        "  model.save(model_path)\n",
        "  print(\"Modèle enregistré dans \", model_path)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/lstm_imdb_model.h5 trouvé !\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 80, 128)           128000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 80, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 276,225\n",
            "Trainable params: 276,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-W8G5C3Dmkw",
        "outputId": "d52b1fa8-2984-4054-b24c-f69c882953ab"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 24s 30ms/step - loss: 0.3801 - accuracy: 0.8292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3c6OCwFXwU"
      },
      "source": [
        "def text_preprocessing(text, model_text_length=maxlen):\n",
        "  text_dict = imdb.get_word_index()\n",
        "  preform_text = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokenized_text = [text_dict[w] if w in text_dict.keys() else 0 for w in preform_text]\n",
        "\n",
        "  # Pour compléter le texte afin d'atteindre la longueur requise\n",
        "  return sequence.pad_sequences([tokenized_text], maxlen=model_text_length)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G-ltioSGkQh",
        "outputId": "3e876c3e-7af2-4531-8d39-092978900816"
      },
      "source": [
        "def pred_pipeline(str_text):\n",
        "  processed_text = text_preprocessing(str_text)\n",
        "  test_res = model.predict(processed_text)\n",
        "  print(\"The text \\\"{}\\\" is likely to be {}. (score :{})\".format(str_text, \"positive\" if test_res < 0.5 else \"negative\", test_res))\n",
        "\n",
        "\n",
        "pred_pipeline(\"A very bad movie and bad actors\")\n",
        "pred_pipeline(\"A good movie and nice actors\")\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text \"A very bad movie and bad actors\" is likely to be negative. (score :[[0.9292416]])\n",
            "The text \"A good movie and nice actors\" is likely to be negative. (score :[[0.73992306]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}